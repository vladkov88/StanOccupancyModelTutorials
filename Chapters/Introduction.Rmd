# Introduction

This section provides a crash course on useful topics and code for using occupancy models with Stan in R. 

```{r echo=FALSE}
knitr::read_chunk('../ChaptersCode/IntroductionCode/introductionCode.R')
```


## A brief overview of occupancy models 

Occupancy models allow for imperfect detection.
For example, if I look out my kitchen window and do not see a robin, why did I not see it?
Maybe the bird was not there or maybe I missed it.
If I look out my kitchen window on a regular basis, I could use an occupancy model to estimate my probability of detection.
Occupancy models can be extended to other situations with imperfect detection as well.

## Perfect detection: The logistic regression

In a world with perfect detection (i.e., $p$ = 1), we would not need occupancy models because the species we are looking for would always be detected if they were present. 
In these situations, we could simply use a logistic regression to model the probability that a site is occupied.
In base R, we would use the `glm()` function to fit this model using a maximum likelihood approach:

```{r glmExample, eval = FALSE}
```

Note, that if we want, we can fit the model using a probit link function rather than a logit:
```{r glmProbit, eval = FALSE}
```


Many [good tutorials exist on the differences between probits and logits](http://lmgtfy.com/?q=probit+vs+logit).
Usually, the models produce similar results, although the logit has a slightly fatter tail. 
Personally, I prefer logits if for no other reason than that is what my PhD advisor used.
The reason for mentioning the logit versus probit is that we need to be explicit about our modeling assumption when moving on to Stan and knowing other options exist help see why. 


The logit model may be also easily coded in Stan as well. 
I would save this script as a `logistic_stan.stan` (**Important note:** Stan models must end with `.stan` with a lower case `s`, otherwise the model will not be complied):

    data {
      int<lower=0> N;
      vector[N] x;
      int<lower=0,upper=1> y[N];
    }
    parameters {
      real alpha;
      real beta;
    }
    model {
      y ~ bernoulli_logit(alpha + beta * x);
    }


After saving this code as `logistic_stan.stan`, we can call the model from R. We'll simulate some data and format it to work with Stan here as well.


```{r logisticRegression, eval = FALSE}
```

A few things to point out.

1. Unlike base R, rstan does not have a nice wrapper function. This means we must use integers as the id variable each observation rather than characters or factors.
2. You will want to increase the number of iterations when actually running the model for use. When troubleshooting, I use 100s. When running for publication, I use 1,000s to 10,000s or more.
3. Notice the Bayesian model diagnostics. If these are unfamiliar to you, I suggest working through BDA3 by Gelman et al. or other introductory Bayesian stats book. The [Shinystan](http://mc-stan.org/users/interfaces/shinystan) interface provides many different Bayesian diagnostics tools that work with Stan.


However, when working with occupancy, we must account for imperfect detection.
Because of this, and the details of how Stan works under the hood, we cannot use the default probability functions for Stan. 
To introduce this topic, we will next look at using the probability `target` in Stan. 
The `+=` opperator is a programming shortcut.
For example `x = x + 2`, can be written as `x += 2`.
Our code now looks like this and is saved in the file `logistic_target_stan.stan`:

    data {
      int<lower=0> N; // Number of samples
      vector[N] x; // predictor vector
      int<lower=0,upper=1> y[N];//response vector
    }
    parameters {
      real alpha; //intercept
      real beta; ///slope
    }
    model {
      for( index in 1:N){
        target += binomial_logit_lpmf( y[index] | 1, alpha + beta * x[index]); 
          }
    }

From both of the above example, there are some important features of Stan to note:

First, unlike R or JAGS, Stan requires us to explicitly declare variables like C++. 
A downside is that the language can be less forgiving.
An upside is that the language makes us be precise and makes it harder to "programming by coincidence".
Rather than simply estimate parameters using a model, we must understand their data structure within the model.
That being said, I've spend many an hour refreshing my linear algebra skills to understand my Stan code and dimensions of objects.
But, the end products were code that I now trust and run quickly.

Second, Stan uses code blocks.
These are defined in the Stan manual.
In a nutshell, the require us to declare the input `data`, the estimated `parameters`, and our `model`.
There are also other types of blocks you can look up in the manual and we might see some of them later. 

Third, we can include comments with `//` or blocks of code with `/* comment here */`.

```{r logit_target, eval = FALSE}
```

**Exercise**: Using the code above, fit a probit regression. You'll need to go to the Stan documentation to figure this out. This exercise will also likely take you 1 hr to a couple of days depending upon your abilities with Stan. 

## Binomial versus Bernoulii and long versus wide data

Another important concept for occupancy modeling is data structure and probability distributions.
The Bernoulli distribution models  one sampling event (often denoted as $K =1$).
For example, we might flip a coin once and record this as a data entry.
The Bernoulli is a special case of a binomial distribution. 
The binomial distribution allows us to have multiple sampling events. 
For example, we might flip a coin 10 times and record the number of heads and tails as their own columns.

For both general data analysis and occupancy modeling, I use both distributions. 
When fitting a simple binomial GLM in R, my modeling choice depends upon the structure of the data.
I use a Bernoulli style input (a vector of 0s and 1s for `y`) if my data has coefficients for each observation or the data was given to me in that format.
I use a binomial style input if my data has been aggregated or I want to avoid [pseudoreplication](https://doi.org/10.2307/1942661) (e.g., the tank is the level of replication rather than the individual).
R has two methods for inputting binomial style data.
First, a matrix of "successes" and "failures" maybe used for `y`.
Second, a vector of probabilities may be used for `y` and a `weight =` option specified. 
Closely relate to these distributions are the data concepts of ["wide versus long data in R"](http://lmgtfy.com/?q=wide+versus+long+data+r) and "aggregate versus raw data".

During this code example, you will see how to fit a model using all three methods as well as how to convert code between wide and long formats. 

```{r demoGLMinput, eval = FALSE}
```

For occurrence models in Stan, we must use the Bernoulli distribution (or Binomial with `K = 1`) for the latent variables because we cannot aggregate the data.
Specifically, we need details about each replicate at a lower level.
For example, We cannot aggregate and say that 3 sites had Robins and 2 sites did not. Instead, we need a vector of of these site-level detentions, for example `c(0, 1, 1, 0, 1, 1, 1)`.
For the lowest level of the occurrence model, I often do use a Bernoulli distribution when I do not have coefficients at the observation-level because there are fewer data entries to aggregate over.
We will see these in later chapters. 

## Matrix notation and occupancy models


Models in R such as `lm()` and `glm()` allow users to input [formuals](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html).
Formulas allow users to input factors and have R convert them to a matrix of dummy variables. 
[`model.matrix()`](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/model.matrix.html) allows us to create these same type matrices of input variables. 
There are several benefits to using `model.matrix()` to pre-process inputs for Stan.
First, it allows us to easily turn factors into [dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)).
Second, it allows us to easily have matrices of predictors, which in turn allows us to us matrix algebra within Stan. 
This section introduces `model.matrix()` so that it will be familiar to us later. 
**Note:** I use shorter matrices than most real applications to save screen space.
 
 
### model.matrix basis 

`model.matrix()` use the `~` (shift-\` on US keyboards) for its input.
In statistical English, this could be read as "predicted by", for example `y ~ x` could be spoken or read as y predicted by x.
The follow example demonstrates how it may be used on a simple factor data.frame:


```{r demoMM, eval = TRUE}
```

Things to notice about `model.matrix()`:

1.  `model.matrix()` converted city to an alphabetical order factor.
2.  The first factor is the first in alphabetically. This order may be [changing the factor order in R](http://lmgtfy.com/?q=change+r+factor+order).
3.  The first factor become a global intercept and the other two levels are compared to this. In the next section, we'll see how to change this.

### Intercept for each level 

If we want an intercept for each factor level, we use a `- 1` in the notation.
```{r demoMMm1, eval = TRUE}
```

If we have multiple factors, we can only estimate intercepts for all of one of the factors. 
For example, if we have months and city, we would need a reference month _or_ reference city. 
Also, notice how order matters.
Most advanced book on regression analysis explains this in greater detail (e.g., [Harrell](http://biostat.mc.vanderbilt.edu/wiki/Main/RmS) or [Gelman and Hill](http://www.stat.columbia.edu/~gelman/arm/)). 

```{r demoMMm1b, eval = FALSE}
```

### numeric vs factors

We can also use numeric inputs with `model.matrix()`
For example, if we input month as a numeric vector, R creates a matrix with month as a numeric column.
If we were using the matrix in a regression, this new column would correspond to a slope estimate. 

```{r numericMatrix, eval = TRUE}
```

Conversely, if we input month as a factor, we get similar results as before.
```{r factorMatrix, eval = TRUE}
```
The purpose of this example is to demonstrate how R can sometimes produce unexpected results, especially if we want a measure of time to correspond to an intercept estimate rather than a slope estimate. 

### Subtract 1 from as.numeric()

Closely related to the above point is a problem I have run into when creating binary response variables in R for use with Stan.
For example, let's say we want to model occupancy for a lake and a river:

```{r m1, eval= TRUE}
```

Using Base R, we could just run `glm()` on this data:

```{r m1glm, eval = FALSE}
```

But, look at what happens if we try and convert `occ` to a binary response:

```{r m1mm, eval = FALSE}
```

Now that you've had a crash course on R topics, let's build our first occupancy model with Stan. 

## Review of log rules and probability 

As you will see in the upcoming chapters, Stan requires marginalizing out discrete latent variables. 
This requires working with probabilities.  
Probabilities are often log transformed to increase numerical stability (or, informally: make the equations easier for the computer to solve) AND to change from multiplication to addition. 
Here are some quick refreshers of log rules:

- $\text{log}(xy) = \text{log}(x) + \text{log}(y)$
- $\text{log}(0) = 1$


For example, let's say we flip a coin once.
The coin has probability $p$ of heads and probability $1-p$ of tails.  
We get a heads, which we call 1. 
If we flip the coin 4 times, our outcome could be 1001. 
The probability of this occurring would be: $p(1-p)(1-p)p$. 
The product may be denoted using the product operator $\prod$ (much like $\sum$ is used for summation) and generalized.  
We have $N$ trials and we index over $i$. 
Trials with heads are considered _successes_, which is denoted with a superscript $s$ = 1. 
Trials with tails have $s = 0$. 
These superscripts make the terms either be themselves (e.g., $p^1 = p$) or 1 (e.g., $p^0 = 1$). 
We can write the probability of the event occurring as $P(y|p)$, which is read as the probability of observation $y$ give probability $p$. 
This leads to the formulation:

$$P(y|p) =  \prod_{i = 1}^N p^{s_i} (1-p)^{1-s_i}.$$

Taking the log of this gives 

$$\text{log}(P(y|p)) =  \sum_{i = 1}^N log(p^{s_i}) + \log( (1-p)^{1-s_i}).$$

Two key takeaways. 
First, notice how the product now became a summation. 
Second, $x \times 1 = x$ and now $x + \text{log(1)} = x + 0 = x$

MacKenzie et al. covers these calculations for occupancy models in chapter 4 of their book. 
